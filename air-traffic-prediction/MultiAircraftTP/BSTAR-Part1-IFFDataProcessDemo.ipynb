{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spatio-Temporal Graph Transformer Network (B-STAR) for Multi-Aircraft Trajectory Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: IFF ASDE-X Flight Track Data Processing\n",
    "Note: This is a demo to show the functionality of large-scale data processing capability. In practice, data processing is performed on the data server via ssh.\n",
    "\n",
    "Author: Yutian Pang, Arizona State University\n",
    "\n",
    "Email: yutian.pang@asu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Requirements\n",
    "\n",
    "This file is test functional with,\n",
    "- Ubuntu 20.04 LTS\n",
    "- Python 3.8.5\n",
    "- Spark 3.1.1 with Hadoop3.2 \n",
    "\n",
    "Other environments should be good but haven't been tested.\n",
    "\n",
    "The required packages are, \n",
    "- [PySpark](http://spark.apache.org/docs/latest/api/python/getting_started/index.html): It's the PYTHON API for Apache Spark with features like distributed processing (with the help of the partitioned ***resilient distributed dataset*** (RDD)) and backend SQL queries for structured data.\n",
    "- [Sedona(GeoSpark)](https://sedona.apache.org/): The geo-spatial extension of PySpark with ***Spatial RDDs*** and Spatial SQL for large-scale distributed geo-spatial data processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The installation with PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyspark sedona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not always required to setup the SPARK environment manually in your system. In rare cases, PySpark cannot find Spark in your system. You will have to set it explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, glob\n",
    "# os.environ[\"SPARK_HOME\"] = '/home/ypang6/spark-3.1.1-bin-hadoop3.2'\n",
    "# os.environ[\"PYTHONPATH\"] = '/home/ypang6/anaconda3/bin/python3.8'\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/ypang6/anaconda3/bin/python3.8'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/ypang6/anaconda3/bin/python3.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test spark environment\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema():\n",
    "    myschema = StructType([\n",
    "        StructField(\"recType\", ShortType(), True),  # 1  //track point record type number\n",
    "        StructField(\"recTime\", StringType(), True),  # 2  //seconds since midnigght 1/1/70 UTC\n",
    "        StructField(\"fltKey\", LongType(), True),  # 3  //flight key\n",
    "        StructField(\"bcnCode\", IntegerType(), True),  # 4  //digit range from 0 to 7\n",
    "        StructField(\"cid\", IntegerType(), True),  # 5  //computer flight id\n",
    "        StructField(\"Source\", StringType(), True),  # 6  //source of the record\n",
    "        StructField(\"msgType\", StringType(), True),  # 7\n",
    "        StructField(\"acId\", StringType(), True),  # 8  //call sign\n",
    "        StructField(\"recTypeCat\", IntegerType(), True),  # 9\n",
    "        StructField(\"lat\", DoubleType(), True),  # 10\n",
    "        StructField(\"lon\", DoubleType(), True),  # 11\n",
    "        StructField(\"alt\", DoubleType(), True),  # 12  //in 100s of feet\n",
    "        StructField(\"significance\", ShortType(), True),  # 13 //digit range from 1 to 10\n",
    "        StructField(\"latAcc\", DoubleType(), True),  # 14\n",
    "        StructField(\"lonAcc\", DoubleType(), True),  # 15\n",
    "        StructField(\"altAcc\", DoubleType(), True),  # 16\n",
    "        StructField(\"groundSpeed\", IntegerType(), True),  # 17 //in knots\n",
    "        StructField(\"course\", DoubleType(), True),  # 18  //in degrees from true north\n",
    "        StructField(\"rateOfClimb\", DoubleType(), True),  # 19  //in feet per minute\n",
    "        StructField(\"altQualifier\", StringType(), True),  # 20  //Altitude qualifier (the “B4 character”)\n",
    "        StructField(\"altIndicator\", StringType(), True),  # 21  //Altitude indicator (the “C4 character”)\n",
    "        StructField(\"trackPtStatus\", StringType(), True),  # 22  //Track point status (e.g., ‘C’ for coast)\n",
    "        StructField(\"leaderDir\", IntegerType(), True),  # 23  //int 0-8 representing the direction of the leader line\n",
    "        StructField(\"scratchPad\", StringType(), True),  # 24\n",
    "        StructField(\"msawInhibitInd\", ShortType(), True),  # 25 // MSAW Inhibit Indicator (0=not inhibited, 1=inhibited)\n",
    "        StructField(\"assignedAltString\", StringType(), True),  # 26\n",
    "        StructField(\"controllingFac\", StringType(), True),  # 27\n",
    "        StructField(\"controllingSec\", StringType(), True),  # 28\n",
    "        StructField(\"receivingFac\", StringType(), True),  # 29\n",
    "        StructField(\"receivingSec\", StringType(), True),  # 30\n",
    "        StructField(\"activeContr\", IntegerType(), True),  # 31  // the active control number\n",
    "        StructField(\"primaryContr\", IntegerType(), True),\n",
    "        # 32  //The primary(previous, controlling, or possible next)controller number\n",
    "        StructField(\"kybrdSubset\", StringType(), True),  # 33  //identifies a subset of controller keyboards\n",
    "        StructField(\"kybrdSymbol\", StringType(), True),  # 34  //identifies a keyboard within the keyboard subsets\n",
    "        StructField(\"adsCode\", IntegerType(), True),  # 35  //arrival departure status code\n",
    "        StructField(\"opsType\", StringType(), True),  # 36  //Operations type (O/E/A/D/I/U)from ARTS and ARTS 3A data\n",
    "        StructField(\"airportCode\", StringType(), True),  # 37\n",
    "        StructField(\"trackNumber\", IntegerType(), True),  # 38\n",
    "        StructField(\"tptReturnType\", StringType(), True),  # 39\n",
    "        StructField(\"modeSCode\", StringType(), True)  # 40\n",
    "    ])\n",
    "    return myschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"Sector_IFF_Parser\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.0.0-incubating,org.datasyslab:geotools-wrapper:geotools-24.0\") .\\\n",
    "        getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n",
    "iff_schema = load_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## We use ASDE-X flight recordings here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# The date of ASDE-X we are going to process\n",
    "date = 20190807\n",
    "\n",
    "# The path to the csv file\n",
    "data_path = \"/PATH_TO_CSV/IFF_ATL+ASDEX_{}*.csv\".format(date)\n",
    "# For example,\n",
    "data_path = \"/media/ypang6/paralab/Research/data/ATL/IFF_ATL+ASDEX_{}*.csv\".format(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(date):\n",
    "    file_path = glob.glob(data_path)[0]\n",
    "    df = spark.read.csv(file_path, header=False, sep=\",\", schema=iff_schema)\n",
    "    print(\"Date: {} Number of Lines: {}\".format(date, df.count()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20190807 Number of Lines: 1699663\n"
     ]
    }
   ],
   "source": [
    "# load data into Sedona\n",
    "df = load_data(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+---------+-----+\n",
      "|recType|   recTime|acId|     lat|      lon|  alt|\n",
      "+-------+----------+----+--------+---------+-----+\n",
      "|      3|1565149748|UNKN|33.63186|-84.44462|10.06|\n",
      "|      3|1565149749|UNKN|33.63186|-84.44455|10.06|\n",
      "|      3|1565149752|UNKN|33.63183|-84.44448|10.06|\n",
      "|      3|1565149753|UNKN|33.63183|-84.44443|10.06|\n",
      "|      3|1565149754|UNKN|33.63183|-84.44438|10.06|\n",
      "+-------+----------+----+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this work, we are only interested in the flight ID, timestamps, and coordinates(latitude, longitude, altitude)\n",
    "cols = ['recType', 'recTime', 'acId', 'lat', 'lon', 'alt']\n",
    "df = df.select(*cols).filter(df['recType']==3).withColumn(\"recTime\", df['recTime'].cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum altitude in the data is 8,500 feet\n",
    "df.agg({\"alt\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spatial Dataframe with Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time window to query\n",
    "duration = 4 # hours # para-atm time window filtering function\n",
    "t_start = 1564668000 + (date-20190801)*24*3600 # Aug 1st, 2pm, 2019, UTC\n",
    "t_end = t_start + 3600*duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------+-------+-----+\n",
      "|geom                      |recTime   |acId   |alt  |\n",
      "+--------------------------+----------+-------+-----+\n",
      "|POINT (33.63759 -84.43789)|1565186555|DAL1350|10.06|\n",
      "|POINT (33.63753 -84.43788)|1565186556|DAL1350|10.06|\n",
      "|POINT (33.63747 -84.43786)|1565186557|DAL1350|10.06|\n",
      "|POINT (33.63737 -84.43784)|1565186558|DAL1350|10.06|\n",
      "|POINT (33.63733 -84.43785)|1565186560|DAL1350|10.06|\n",
      "+--------------------------+----------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register pyspark df in SQL\n",
    "df.registerTempTable(\"pointtable\")  # now we have a registered SQL table running backened in the system\n",
    "\n",
    "# create shape column in geospark\n",
    "spatialdf = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT ST_Point(CAST(lat AS Decimal(24, 20)), CAST(lon AS Decimal(24, 20))) AS geom, recTime, acId, alt\n",
    "  FROM pointtable\n",
    "  WHERE recTime>={} AND recTime<={}\n",
    "  \"\"\".format(t_start, t_end))\n",
    "\n",
    "spatialdf.createOrReplaceTempView(\"spatialdf\")\n",
    "spatialdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of points after the first query\n",
    "spatialdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Rectangular Query around KATL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "katl = [33.6366996, -84.4278640, 10.06]  # https://www.airnav.com/airport/katl\n",
    "r = 0.2 # rectangular query range\n",
    "vs = 0.3 # vertical threshold unit: x100 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n",
    "range_query_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT DISTINCT acId\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]+vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of flight IDs after the second query\n",
    "range_query_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   acId|\n",
      "+-------+\n",
      "|DAL2041|\n",
      "| UAL241|\n",
      "|GJS4507|\n",
      "|DAL1154|\n",
      "|SKW3742|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_query_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152508"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT *\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]))\n",
    "\n",
    "# Count the number of points after the second query\n",
    "df_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymization\n",
    "\n",
    "Anonymization is a type of data sanitization technique to remove identifiable information from the data. In this work, we perform two operations to anonymize the data while retaining useful geo-spatial features.\n",
    "\n",
    "- Normalize the timestamp by the earliest time in the current dataframe.\n",
    "- Mask the real flight IDs into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------------+-----+\n",
      "|acId   |t  |geom                      |alt  |\n",
      "+-------+---+--------------------------+-----+\n",
      "|DAL1350|448|POINT (33.63582 -84.41124)|10.38|\n",
      "|DAL1350|449|POINT (33.63583 -84.41117)|10.5 |\n",
      "|DAL1350|450|POINT (33.63576 -84.41111)|10.25|\n",
      "|DAL1350|451|POINT (33.63577 -84.41103)|10.13|\n",
      "|DAL1350|665|POINT (33.63472 -84.41763)|10.75|\n",
      "+-------+---+--------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create relevant timestamp column\n",
    "df_result.createOrReplaceTempView(\"spatialdf\")\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT acId, recTime-{} AS t, geom, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(t_start)\n",
    ")\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------+-----+-----+\n",
      "|t  |geom                      |alt  |FacId|\n",
      "+---+--------------------------+-----+-----+\n",
      "|448|POINT (33.63582 -84.41124)|10.38|56.0 |\n",
      "|449|POINT (33.63583 -84.41117)|10.5 |56.0 |\n",
      "|450|POINT (33.63576 -84.41111)|10.25|56.0 |\n",
      "|451|POINT (33.63577 -84.41103)|10.13|56.0 |\n",
      "|665|POINT (33.63472 -84.41763)|10.75|56.0 |\n",
      "|666|POINT (33.63471 -84.41828)|10.56|56.0 |\n",
      "|667|POINT (33.6347 -84.41892) |10.31|56.0 |\n",
      "|668|POINT (33.6347 -84.41959) |10.38|56.0 |\n",
      "|669|POINT (33.6347 -84.42027) |10.38|56.0 |\n",
      "|670|POINT (33.63468 -84.421)  |10.38|56.0 |\n",
      "|671|POINT (33.63467 -84.42173)|10.38|56.0 |\n",
      "|672|POINT (33.6347 -84.42251) |10.38|56.0 |\n",
      "|673|POINT (33.6347 -84.42328) |10.38|56.0 |\n",
      "|674|POINT (33.63471 -84.42409)|10.38|56.0 |\n",
      "|675|POINT (33.6347 -84.42492) |10.38|56.0 |\n",
      "|676|POINT (33.63469 -84.42577)|10.38|56.0 |\n",
      "|677|POINT (33.6347 -84.42663) |10.38|56.0 |\n",
      "|678|POINT (33.63472 -84.4275) |10.38|56.0 |\n",
      "|679|POINT (33.63472 -84.42841)|10.13|56.0 |\n",
      "|682|POINT (33.63469 -84.43121)|10.38|56.0 |\n",
      "+---+--------------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change acId Into integers\n",
    "# have to use pyspark ml features\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"acId\", outputCol=\"FacId\")\n",
    "df_new = indexer.fit(df).transform(df).drop('acId')\n",
    "df_new.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, CAST(FacId AS Integer), ST_X(geom) as lat, ST_Y(geom) as lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data after anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+---------+-----+\n",
      "|t  |FacId|lat     |lon      |alt  |\n",
      "+---+-----+--------+---------+-----+\n",
      "|448|56   |33.63582|-84.41124|10.38|\n",
      "|449|56   |33.63583|-84.41117|10.5 |\n",
      "|450|56   |33.63576|-84.41111|10.25|\n",
      "|451|56   |33.63577|-84.41103|10.13|\n",
      "|665|56   |33.63472|-84.41763|10.75|\n",
      "+---+-----+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some small modifications to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 1: \n",
    "#### Resample the time series with an interval $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10\n",
    "t_start = 0\n",
    "t_end = 3600 * duration\n",
    "\n",
    "t_interval = list(range(t_start, t_end, dt))\n",
    "df = df[df.t.isin(t_interval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+---------+-----+\n",
      "|  t|FacId|     lat|      lon|  alt|\n",
      "+---+-----+--------+---------+-----+\n",
      "|450|   56|33.63576|-84.41111|10.25|\n",
      "|670|   56|33.63468|  -84.421|10.38|\n",
      "|690|   56|33.63457|-84.43889|14.06|\n",
      "|700|   56| 33.6344|-84.44845|19.13|\n",
      "|710|   56| 33.6344|-84.45818| 23.5|\n",
      "|720|   56|33.63442|-84.46875|25.31|\n",
      "|730|   56|33.63246|   -84.48| 28.5|\n",
      "|740|   56|33.62812|-84.49124|31.06|\n",
      "|750|   56| 33.6228|-84.50281|34.63|\n",
      "|760|   56|33.61735|-84.51479|39.31|\n",
      "|770|   56|33.61187|-84.52725|44.13|\n",
      "|780|   56|33.60656|-84.54127|48.63|\n",
      "|790|   56|33.60111|-84.55442|54.25|\n",
      "|800|   56|33.59564|-84.56774|59.44|\n",
      "|810|   56| 33.5874|-84.57992| 64.0|\n",
      "+---+-----+--------+---------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 2:\n",
    "Change the origin of the coordinate system to the airport center\n",
    "- void in real case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df2 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, FacId, lat-{} AS Lat, lon-{} AS Lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(katl[0], katl[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'int'),\n",
       " ('FacId', 'int'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double'),\n",
       " ('alt', 'double')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the data types are correct\n",
    "df.toPandas()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data with altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}_wAltitude.csv'.format(r, date, duration)\n",
    "df.toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')\n",
    "\n",
    "# save data without altitude dimension\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}.csv'.format(r, date, duration)\n",
    "df.drop('alt').toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd005fcb5bad7999e823fc09222c9f731485df3b4df5c9c8c16483746be34be6188"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
