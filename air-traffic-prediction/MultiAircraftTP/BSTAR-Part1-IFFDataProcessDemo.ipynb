{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spatio-Temporal Graph Transformer Network (B-STAR) for Multi-Aircraft Trajectory Prediction\n",
    "Author: Yutian Pang, Arizona State University\n",
    "\n",
    "\n",
    "Email: yutian.pang@asu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: IFF ASDE-X Flight Track Data Processing with PySpark and Hadoop\n",
    "This is a demonstration of the process of using PySpark and Hadoop for large-scale processing of IFF ASDE-X data. In practice, this data processing would be performed on a server via ssh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Requirements\n",
    "\n",
    "This Jupyter notebook has been tested with:\n",
    "- Ubuntu 20.04 LTS (and 18.04 LTS)\n",
    "- Python 3.8.5 (and 3.8.10)\n",
    "- Spark 3.1.1 with Hadoop3.2 (and Spark 3.2.1 with Hadoop3.2)\n",
    "\n",
    "The software in parenthesis were tested together. Other combinations of Ubuntu, Python, and Spark should be verified for compatibility. See [this](https://stackoverflow.com/questions/58700384/how-to-fix-typeerror-an-integer-is-required-got-type-bytes-error-when-tryin) article for further guidance.\n",
    "\n",
    "### Windows 10 Users\n",
    "#### Installing Ubuntu on Windows 10 with Windows Subsystem for LInux (WSL)\n",
    "Windows 10 users with admin privileges can enable Windows Subsystem for Linux (WSL) following [these](https://docs.microsoft.com/en-us/windows/wsl/install-win10) directions. \n",
    "\n",
    "\n",
    "#### Installing Anaconda on WSL\n",
    "A user can then install Anaconda on their WSL Ubuntu distribution following [these](https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da) instructions.\n",
    "\n",
    "#### Download and unzipping Spark on WSL\n",
    "Identify the distribution of Spark and Hadoop you require [here](https://spark.apache.org/downloads.html). In your Ubuntu terminal window execute the ```wget``` command followed by the download link in your chosen download directory (likely the ```HOME``` directory). Then, unzip the downloaded .tgz file with ```tar -xvzf [fname]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required Python packages\n",
    "The required Python packages for this module are:\n",
    "- [```pyspark```](http://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "    - This is the Python API for Apache Spark. We will be using the distributed processing features (with the help of the partitioned ***resilient distributed dataset*** (RDD)) and backend SQL queries for structured data.\n",
    "- [```sedona```](https://sedona.apache.org/)\n",
    "    - This is the geo-spatial extension of PySpark with ***Spatial RDDs*** and Spatial SQL for large-scale distributed geo-spatial data processing.\n",
    "    \n",
    "In the Ubuntu or Anaconda terminal, execute ```pip install pyspark apache-sedona```. This will install both the ```pyspark``` and ```sedona``` packages. \n",
    "\n",
    "See more on the installati```sedona```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "The Spark codes (note: Improve this description) retrieve the ```SPARK_HOME```, ```PYTHONPATH```, ```PYSPARK_PYTHON```, and ```PYSPARK_DRIVER_PYTHON``` system variables. Either (Option 1) these are set in the shell environment in the ```.bash_profile``` script or (Option 2) in the Python script prior to calling the ```pyspark``` module.\n",
    "### Option 1: Add environment variables to the ```.bash_profile``` script\n",
    "Open the ```.bash_profile``` script in your text editor. On Ubuntu systems, this script is usually found in your ```HOME``` directory ```~/```. If this file does not yet exist (or is empty) you can create one. Then add the following ```export``` statements for each variable you want to add and add them to the path. For example:\n",
    "\n",
    "```export SPARK_HOME=\"$HOME/spark-X.X.X-bin-hadoopX.X\"```\n",
    "\n",
    "```export PYTHONPATH=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "```export PYSPARK_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "```export PYSPARK_DRIVER_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "```export PATH=\"$SPARK_HOME/bin:$PATH\"```\n",
    "\n",
    "\n",
    "### Option 2: Add the environment variables in the Python script using the ```os``` package\n",
    "```import os```\n",
    "\n",
    "```os.environ[\"SPARK_HOME\"] = '/home/ypang6/spark-3.1.1-bin-hadoop3.2'```\n",
    "\n",
    "```os.environ[\"PYTHONPATH\"] = '/home/ypang6/anaconda3/bin/python3.8'```\n",
    "\n",
    "```os.environ['PYSPARK_PYTHON'] = '/home/ypang6/anaconda3/bin/python3.8'```\n",
    "\n",
    "```os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/ypang6/anaconda3/bin/python3.8'```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SQL Schema with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Spark Session module\n",
    "# We need SQL utilities from Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (ShortType, StringType, StructType,StructField,LongType, IntegerType, DoubleType)\n",
    "\n",
    "def load_schema():\n",
    "    myschema = StructType([\n",
    "        StructField(\"recType\", ShortType(), True),  # 1  //track point record type number\n",
    "        StructField(\"recTime\", StringType(), True),  # 2  //seconds since midnigght 1/1/70 UTC\n",
    "        StructField(\"fltKey\", LongType(), True),  # 3  //flight key\n",
    "        StructField(\"bcnCode\", IntegerType(), True),  # 4  //digit range from 0 to 7\n",
    "        StructField(\"cid\", IntegerType(), True),  # 5  //computer flight id\n",
    "        StructField(\"Source\", StringType(), True),  # 6  //source of the record\n",
    "        StructField(\"msgType\", StringType(), True),  # 7\n",
    "        StructField(\"acId\", StringType(), True),  # 8  //call sign\n",
    "        StructField(\"recTypeCat\", IntegerType(), True),  # 9\n",
    "        StructField(\"lat\", DoubleType(), True),  # 10\n",
    "        StructField(\"lon\", DoubleType(), True),  # 11\n",
    "        StructField(\"alt\", DoubleType(), True),  # 12  //in 100s of feet\n",
    "        StructField(\"significance\", ShortType(), True),  # 13 //digit range from 1 to 10\n",
    "        StructField(\"latAcc\", DoubleType(), True),  # 14\n",
    "        StructField(\"lonAcc\", DoubleType(), True),  # 15\n",
    "        StructField(\"altAcc\", DoubleType(), True),  # 16\n",
    "        StructField(\"groundSpeed\", IntegerType(), True),  # 17 //in knots\n",
    "        StructField(\"course\", DoubleType(), True),  # 18  //in degrees from true north\n",
    "        StructField(\"rateOfClimb\", DoubleType(), True),  # 19  //in feet per minute\n",
    "        StructField(\"altQualifier\", StringType(), True),  # 20  //Altitude qualifier (the “B4 character”)\n",
    "        StructField(\"altIndicator\", StringType(), True),  # 21  //Altitude indicator (the “C4 character”)\n",
    "        StructField(\"trackPtStatus\", StringType(), True),  # 22  //Track point status (e.g., ‘C’ for coast)\n",
    "        StructField(\"leaderDir\", IntegerType(), True),  # 23  //int 0-8 representing the direction of the leader line\n",
    "        StructField(\"scratchPad\", StringType(), True),  # 24\n",
    "        StructField(\"msawInhibitInd\", ShortType(), True),  # 25 // MSAW Inhibit Indicator (0=not inhibited, 1=inhibited)\n",
    "        StructField(\"assignedAltString\", StringType(), True),  # 26\n",
    "        StructField(\"controllingFac\", StringType(), True),  # 27\n",
    "        StructField(\"controllingSec\", StringType(), True),  # 28\n",
    "        StructField(\"receivingFac\", StringType(), True),  # 29\n",
    "        StructField(\"receivingSec\", StringType(), True),  # 30\n",
    "        StructField(\"activeContr\", IntegerType(), True),  # 31  // the active control number\n",
    "        StructField(\"primaryContr\", IntegerType(), True),\n",
    "        # 32  //The primary(previous, controlling, or possible next)controller number\n",
    "        StructField(\"kybrdSubset\", StringType(), True),  # 33  //identifies a subset of controller keyboards\n",
    "        StructField(\"kybrdSymbol\", StringType(), True),  # 34  //identifies a keyboard within the keyboard subsets\n",
    "        StructField(\"adsCode\", IntegerType(), True),  # 35  //arrival departure status code\n",
    "        StructField(\"opsType\", StringType(), True),  # 36  //Operations type (O/E/A/D/I/U)from ARTS and ARTS 3A data\n",
    "        StructField(\"airportCode\", StringType(), True),  # 37\n",
    "        StructField(\"trackNumber\", IntegerType(), True),  # 38\n",
    "        StructField(\"tptReturnType\", StringType(), True),  # 39\n",
    "        StructField(\"modeSCode\", StringType(), True)  # 40\n",
    "    ])\n",
    "    return myschema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sedona to register Spark Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"Sector_IFF_Parser\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.0.0-incubating,org.datasyslab:geotools-wrapper:geotools-24.0\") .\\\n",
    "        getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## We use ASDE-X flight recordings here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "iff_schema = load_schema()\n",
    "# The date of ASDE-X we are going to process\n",
    "date = 20190807\n",
    "\n",
    "# The path to the csv file\n",
    "data_path = \"/PATH_TO_CSV/IFF_ATL+ASDEX_{}*.csv\".format(date)\n",
    "# For example,\n",
    "data_path = \"/media/ypang6/paralab/Research/data/ATL/IFF_ATL+ASDEX_{}*.csv\".format(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(date):\n",
    "    file_path = glob.glob(data_path)[0]\n",
    "    df = spark.read.csv(file_path, header=False, sep=\",\", schema=iff_schema)\n",
    "    print(\"Date: {} Number of Lines: {}\".format(date, df.count()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20190807 Number of Lines: 1699663\n"
     ]
    }
   ],
   "source": [
    "# load data into Sedona\n",
    "df = load_data(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+---------+-----+\n",
      "|recType|   recTime|acId|     lat|      lon|  alt|\n",
      "+-------+----------+----+--------+---------+-----+\n",
      "|      3|1565149748|UNKN|33.63186|-84.44462|10.06|\n",
      "|      3|1565149749|UNKN|33.63186|-84.44455|10.06|\n",
      "|      3|1565149752|UNKN|33.63183|-84.44448|10.06|\n",
      "|      3|1565149753|UNKN|33.63183|-84.44443|10.06|\n",
      "|      3|1565149754|UNKN|33.63183|-84.44438|10.06|\n",
      "+-------+----------+----+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this work, we are only interested in the flight ID, timestamps, and coordinates(latitude, longitude, altitude)\n",
    "cols = ['recType', 'recTime', 'acId', 'lat', 'lon', 'alt']\n",
    "df = df.select(*cols).filter(df['recType']==3).withColumn(\"recTime\", df['recTime'].cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum altitude in the data is 8,500 feet\n",
    "df.agg({\"alt\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spatial Dataframe with Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time window to query\n",
    "duration = 4 # hours # para-atm time window filtering function\n",
    "t_start = 1564668000 + (date-20190801)*24*3600 # Aug 1st, 2pm, 2019, UTC\n",
    "t_end = t_start + 3600*duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------+-------+-----+\n",
      "|geom                      |recTime   |acId   |alt  |\n",
      "+--------------------------+----------+-------+-----+\n",
      "|POINT (33.63759 -84.43789)|1565186555|DAL1350|10.06|\n",
      "|POINT (33.63753 -84.43788)|1565186556|DAL1350|10.06|\n",
      "|POINT (33.63747 -84.43786)|1565186557|DAL1350|10.06|\n",
      "|POINT (33.63737 -84.43784)|1565186558|DAL1350|10.06|\n",
      "|POINT (33.63733 -84.43785)|1565186560|DAL1350|10.06|\n",
      "+--------------------------+----------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register pyspark df in SQL\n",
    "df.registerTempTable(\"pointtable\")  # now we have a registered SQL table running backened in the system\n",
    "\n",
    "# create shape column in geospark\n",
    "spatialdf = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT ST_Point(CAST(lat AS Decimal(24, 20)), CAST(lon AS Decimal(24, 20))) AS geom, recTime, acId, alt\n",
    "  FROM pointtable\n",
    "  WHERE recTime>={} AND recTime<={}\n",
    "  \"\"\".format(t_start, t_end))\n",
    "\n",
    "spatialdf.createOrReplaceTempView(\"spatialdf\")\n",
    "spatialdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of points after the first query\n",
    "spatialdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Rectangular Query around KATL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "katl = [33.6366996, -84.4278640, 10.06]  # https://www.airnav.com/airport/katl\n",
    "r = 0.2 # rectangular query range\n",
    "vs = 0.3 # vertical threshold unit: x100 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n",
    "range_query_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT DISTINCT acId\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]+vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of flight IDs after the second query\n",
    "range_query_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   acId|\n",
      "+-------+\n",
      "|DAL2041|\n",
      "| UAL241|\n",
      "|GJS4507|\n",
      "|DAL1154|\n",
      "|SKW3742|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_query_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152508"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT *\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]))\n",
    "\n",
    "# Count the number of points after the second query\n",
    "df_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymization\n",
    "\n",
    "Anonymization is a type of data sanitization technique to remove identifiable information from the data. In this work, we perform two operations to anonymize the data while retaining useful geo-spatial features.\n",
    "\n",
    "- Normalize the timestamp by the earliest time in the current dataframe.\n",
    "- Mask the real flight IDs into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------------+-----+\n",
      "|acId   |t  |geom                      |alt  |\n",
      "+-------+---+--------------------------+-----+\n",
      "|DAL1350|448|POINT (33.63582 -84.41124)|10.38|\n",
      "|DAL1350|449|POINT (33.63583 -84.41117)|10.5 |\n",
      "|DAL1350|450|POINT (33.63576 -84.41111)|10.25|\n",
      "|DAL1350|451|POINT (33.63577 -84.41103)|10.13|\n",
      "|DAL1350|665|POINT (33.63472 -84.41763)|10.75|\n",
      "+-------+---+--------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create relevant timestamp column\n",
    "df_result.createOrReplaceTempView(\"spatialdf\")\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT acId, recTime-{} AS t, geom, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(t_start)\n",
    ")\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------+-----+-----+\n",
      "|t  |geom                      |alt  |FacId|\n",
      "+---+--------------------------+-----+-----+\n",
      "|448|POINT (33.63582 -84.41124)|10.38|56.0 |\n",
      "|449|POINT (33.63583 -84.41117)|10.5 |56.0 |\n",
      "|450|POINT (33.63576 -84.41111)|10.25|56.0 |\n",
      "|451|POINT (33.63577 -84.41103)|10.13|56.0 |\n",
      "|665|POINT (33.63472 -84.41763)|10.75|56.0 |\n",
      "|666|POINT (33.63471 -84.41828)|10.56|56.0 |\n",
      "|667|POINT (33.6347 -84.41892) |10.31|56.0 |\n",
      "|668|POINT (33.6347 -84.41959) |10.38|56.0 |\n",
      "|669|POINT (33.6347 -84.42027) |10.38|56.0 |\n",
      "|670|POINT (33.63468 -84.421)  |10.38|56.0 |\n",
      "|671|POINT (33.63467 -84.42173)|10.38|56.0 |\n",
      "|672|POINT (33.6347 -84.42251) |10.38|56.0 |\n",
      "|673|POINT (33.6347 -84.42328) |10.38|56.0 |\n",
      "|674|POINT (33.63471 -84.42409)|10.38|56.0 |\n",
      "|675|POINT (33.6347 -84.42492) |10.38|56.0 |\n",
      "|676|POINT (33.63469 -84.42577)|10.38|56.0 |\n",
      "|677|POINT (33.6347 -84.42663) |10.38|56.0 |\n",
      "|678|POINT (33.63472 -84.4275) |10.38|56.0 |\n",
      "|679|POINT (33.63472 -84.42841)|10.13|56.0 |\n",
      "|682|POINT (33.63469 -84.43121)|10.38|56.0 |\n",
      "+---+--------------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change acId Into integers\n",
    "# have to use pyspark ml features\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"acId\", outputCol=\"FacId\")\n",
    "df_new = indexer.fit(df).transform(df).drop('acId')\n",
    "df_new.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, CAST(FacId AS Integer), ST_X(geom) as lat, ST_Y(geom) as lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data after anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+---------+-----+\n",
      "|t  |FacId|lat     |lon      |alt  |\n",
      "+---+-----+--------+---------+-----+\n",
      "|448|56   |33.63582|-84.41124|10.38|\n",
      "|449|56   |33.63583|-84.41117|10.5 |\n",
      "|450|56   |33.63576|-84.41111|10.25|\n",
      "|451|56   |33.63577|-84.41103|10.13|\n",
      "|665|56   |33.63472|-84.41763|10.75|\n",
      "+---+-----+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some small modifications to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 1: \n",
    "#### Resample the time series with an interval $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10\n",
    "t_start = 0\n",
    "t_end = 3600 * duration\n",
    "\n",
    "t_interval = list(range(t_start, t_end, dt))\n",
    "df = df[df.t.isin(t_interval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+---------+-----+\n",
      "|  t|FacId|     lat|      lon|  alt|\n",
      "+---+-----+--------+---------+-----+\n",
      "|450|   56|33.63576|-84.41111|10.25|\n",
      "|670|   56|33.63468|  -84.421|10.38|\n",
      "|690|   56|33.63457|-84.43889|14.06|\n",
      "|700|   56| 33.6344|-84.44845|19.13|\n",
      "|710|   56| 33.6344|-84.45818| 23.5|\n",
      "|720|   56|33.63442|-84.46875|25.31|\n",
      "|730|   56|33.63246|   -84.48| 28.5|\n",
      "|740|   56|33.62812|-84.49124|31.06|\n",
      "|750|   56| 33.6228|-84.50281|34.63|\n",
      "|760|   56|33.61735|-84.51479|39.31|\n",
      "|770|   56|33.61187|-84.52725|44.13|\n",
      "|780|   56|33.60656|-84.54127|48.63|\n",
      "|790|   56|33.60111|-84.55442|54.25|\n",
      "|800|   56|33.59564|-84.56774|59.44|\n",
      "|810|   56| 33.5874|-84.57992| 64.0|\n",
      "+---+-----+--------+---------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 2:\n",
    "Change the origin of the coordinate system to the airport center\n",
    "- void in real case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df2 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, FacId, lat-{} AS Lat, lon-{} AS Lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(katl[0], katl[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'int'),\n",
       " ('FacId', 'int'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double'),\n",
       " ('alt', 'double')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the data types are correct\n",
    "df.toPandas()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data with altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}_wAltitude.csv'.format(r, date, duration)\n",
    "df.toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')\n",
    "\n",
    "# save data without altitude dimension\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}.csv'.format(r, date, duration)\n",
    "df.drop('alt').toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
