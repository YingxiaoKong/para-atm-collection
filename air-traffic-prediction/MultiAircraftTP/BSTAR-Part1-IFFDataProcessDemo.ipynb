{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spatio-Temporal Graph Transformer Network (B-STAR) for Multi-Aircraft Trajectory Prediction\n",
    "Author: Yutian Pang, Arizona State University\n",
    "\n",
    "\n",
    "Email: yutian.pang@asu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: IFF ASDE-X Flight Track Data Processing with PySpark and Hadoop\n",
    "This is a demonstration of using PySpark and Hadoop for large-scale processing of IFF ASDE-X data. In practice, this data processing would be performed on a server or high-performance cluster via ssh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Requirements\n",
    "\n",
    "This Jupyter notebook has been tested with:\n",
    "- Ubuntu 20.04 LTS (and 18.04 LTS)\n",
    "- Python 3.8.5 (and 3.8.10)\n",
    "- Spark 3.1.1 with Hadoop3.2 (and Spark 3.2.1 with Hadoop3.2)\n",
    "\n",
    "The software in parenthesis were tested together. Other combinations of Ubuntu, Python, and Spark should be verified for compatibility. See [this](https://stackoverflow.com/questions/58700384/how-to-fix-typeerror-an-integer-is-required-got-type-bytes-error-when-tryin) article for further guidance.\n",
    "\n",
    "### Instructions for Windows 10 Users\n",
    "\n",
    "1. **Install Ubuntu on Windows 10 with Windows Subsystem for LInux (WSL)**\n",
    "    - Windows 10 users with admin privileges can enable Windows Subsystem for Linux (WSL) following [these](https://docs.microsoft.com/en-us/windows/wsl/install-win10) directions.\n",
    "        \n",
    "        \n",
    "2. **Install Anaconda in the Ubuntu terminal**\n",
    "    - A user can then install Anaconda on their WSL Ubuntu distribution following [these](https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da) instructions. \n",
    "        \n",
    "        \n",
    "3. **Download and unzip Spark on WSL**\n",
    "    - Identify the distribution of Spark and Hadoop you require [here](https://spark.apache.org/downloads.html). \n",
    "    - In your Ubuntu terminal window execute the ```wget``` command followed by the download link in your chosen download directory (likely the ```HOME``` directory). \n",
    "    - Then, unzip the downloaded .tgz file with ```tar -xvzf [fname]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required Python packages\n",
    "The required Python packages for this module are:\n",
    "- **[```pyspark```]**(http://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "    - This is the Python API for Apache Spark. We will be using the distributed processing features and backend SQL queries for structured data.\n",
    "- **[```apache-sedona```]**(https://sedona.apache.org/)\n",
    "    - Formerly Geospark, Apache Sedona extends the Resilient Distributed Dataset (RDD), the core data structure in Apache Spark, to accommodate big geospatial data in a cluster environment.\n",
    "    \n",
    "In the Ubuntu or Anaconda terminal, execute ```pip install pyspark apache-sedona```. This will install both the ```pyspark``` and ```apache-sedona``` packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "The Spark codes (note: Improve this description) retrieve the ```SPARK_HOME```, ```PYTHONPATH```, ```PYSPARK_PYTHON```, and ```PYSPARK_DRIVER_PYTHON``` system variables. Either (Option 1) these are set in the shell environment in the ```.bash_profile``` script or (Option 2) in the Python script prior to calling the ```pyspark``` module.\n",
    "- ### Option 1: Add environment variables to the ```.bash_profile``` script\n",
    "\n",
    "    Open the ```.bash_profile``` script in your text editor. On Ubuntu systems, this script is usually found in your ```HOME``` directory ```~/```. If this file does not yet exist (or is empty) you can create one. Then add the following ```export``` statements for each variable you want to add and add them to the path. For example:\n",
    "\n",
    "    ```export SPARK_HOME=\"$HOME/spark-X.X.X-bin-hadoopX.X\"```\n",
    "\n",
    "    ```export PYTHONPATH=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_DRIVER_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PATH=\"$SPARK_HOME/bin:$PATH\"```\n",
    "\n",
    "- ### Option 2: Add the environment variables in the Python script using the ```os``` package\n",
    "\n",
    "    ```import os```\n",
    "       \n",
    "    ```os.environ[\"SPARK_HOME\"] = '~/spark-3.1.1-bin-hadoop3.2'```\n",
    "\n",
    "    ```os.environ[\"PYTHONPATH\"] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_PYTHON'] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_DRIVER_PYTHON'] = '~/anaconda3/bin/python3.8'```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define SQL Schema for IFF ASDE-X Data with ```pyspark``` Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve several different SQL types from pyspark\n",
    "from pyspark.sql.types import (ShortType, StringType, StructType,StructField,LongType, IntegerType, DoubleType)\n",
    "\n",
    "#Create load_schema function which returns variable 'myschema' specifically for recTye=3.\n",
    "def load_schema():\n",
    "    myschema = StructType([\n",
    "        StructField(\"recType\", ShortType(), True),  # 1  //track point record type number\n",
    "        StructField(\"recTime\", StringType(), True),  # 2  //seconds since midnigght 1/1/70 UTC\n",
    "        StructField(\"fltKey\", LongType(), True),  # 3  //flight key\n",
    "        StructField(\"bcnCode\", IntegerType(), True),  # 4  //digit range from 0 to 7\n",
    "        StructField(\"cid\", IntegerType(), True),  # 5  //computer flight id\n",
    "        StructField(\"Source\", StringType(), True),  # 6  //source of the record\n",
    "        StructField(\"msgType\", StringType(), True),  # 7\n",
    "        StructField(\"acId\", StringType(), True),  # 8  //call sign\n",
    "        StructField(\"recTypeCat\", IntegerType(), True),  # 9\n",
    "        StructField(\"lat\", DoubleType(), True),  # 10\n",
    "        StructField(\"lon\", DoubleType(), True),  # 11\n",
    "        StructField(\"alt\", DoubleType(), True),  # 12  //in 100s of feet\n",
    "        StructField(\"significance\", ShortType(), True),  # 13 //digit range from 1 to 10\n",
    "        StructField(\"latAcc\", DoubleType(), True),  # 14\n",
    "        StructField(\"lonAcc\", DoubleType(), True),  # 15\n",
    "        StructField(\"altAcc\", DoubleType(), True),  # 16\n",
    "        StructField(\"groundSpeed\", IntegerType(), True),  # 17 //in knots\n",
    "        StructField(\"course\", DoubleType(), True),  # 18  //in degrees from true north\n",
    "        StructField(\"rateOfClimb\", DoubleType(), True),  # 19  //in feet per minute\n",
    "        StructField(\"altQualifier\", StringType(), True),  # 20  //Altitude qualifier (the “B4 character”)\n",
    "        StructField(\"altIndicator\", StringType(), True),  # 21  //Altitude indicator (the “C4 character”)\n",
    "        StructField(\"trackPtStatus\", StringType(), True),  # 22  //Track point status (e.g., ‘C’ for coast)\n",
    "        StructField(\"leaderDir\", IntegerType(), True),  # 23  //int 0-8 representing the direction of the leader line\n",
    "        StructField(\"scratchPad\", StringType(), True),  # 24\n",
    "        StructField(\"msawInhibitInd\", ShortType(), True),  # 25 // MSAW Inhibit Indicator (0=not inhibited, 1=inhibited)\n",
    "        StructField(\"assignedAltString\", StringType(), True),  # 26\n",
    "        StructField(\"controllingFac\", StringType(), True),  # 27\n",
    "        StructField(\"controllingSec\", StringType(), True),  # 28\n",
    "        StructField(\"receivingFac\", StringType(), True),  # 29\n",
    "        StructField(\"receivingSec\", StringType(), True),  # 30\n",
    "        StructField(\"activeContr\", IntegerType(), True),  # 31  // the active control number\n",
    "        StructField(\"primaryContr\", IntegerType(), True),\n",
    "        # 32  //The primary(previous, controlling, or possible next)controller number\n",
    "        StructField(\"kybrdSubset\", StringType(), True),  # 33  //identifies a subset of controller keyboards\n",
    "        StructField(\"kybrdSymbol\", StringType(), True),  # 34  //identifies a keyboard within the keyboard subsets\n",
    "        StructField(\"adsCode\", IntegerType(), True),  # 35  //arrival departure status code\n",
    "        StructField(\"opsType\", StringType(), True),  # 36  //Operations type (O/E/A/D/I/U)from ARTS and ARTS 3A data\n",
    "        StructField(\"airportCode\", StringType(), True),  # 37\n",
    "        StructField(\"trackNumber\", IntegerType(), True),  # 38\n",
    "        StructField(\"tptReturnType\", StringType(), True),  # 39\n",
    "        StructField(\"modeSCode\", StringType(), True)  # 40\n",
    "    ])\n",
    "    return myschema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use Sedona to register Spark Objects"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"Sector_IFF_Parser\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "        config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.1-incubating,'\n",
    "           'org.datasyslab:geotools-wrapper:1.1.0-25.2'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## We use ASDE-X flight recordings here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the demonstration IFF ASDE-X data\n",
    "from paraatm.io.iff import read_iff_file\n",
    "\n",
    "iff_fpath = \"../../miscellaneous/gnats-fpgen/IFF_SFO_ASDEX_ABC456.csv\"\n",
    "\n",
    "#Current paraatm function\n",
    "%timeit df_patm = read_iff_file(iff_fpath)\n",
    "df_patm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iff_schema = load_schema()\n",
    "def load_data(fname,verbose=False):\n",
    "    df = spark.read.csv(fname, header=False, sep=\",\", schema=iff_schema)\n",
    "    if verbose:\n",
    "        print(\"Number of Lines: {}\".format(df.count()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into Sedona\n",
    "%timeit df = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this work, we are only interested in the flight ID, timestamps, and coordinates(latitude, longitude, altitude)\n",
    "cols = ['recType', 'recTime', 'acId', 'lat', 'lon', 'alt']\n",
    "df = df.select(*cols).filter(df['recType']==3).withColumn(\"recTime\", df['recTime'].cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spatial Dataframe with Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time window to query\n",
    "duration = 4 # hours # para-atm time window filtering function\n",
    "t_start = 1564668000 + (date-20190801)*24*3600 # Aug 1st, 2pm, 2019, UTC\n",
    "t_end = t_start + 3600*duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register pyspark df in SQL\n",
    "df.registerTempTable(\"pointtable\")  # now we have a registered SQL table running backened in the system\n",
    "\n",
    "# create shape column in geospark\n",
    "spatialdf = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT ST_Point(CAST(lat AS Decimal(24, 20)), CAST(lon AS Decimal(24, 20))) AS geom, recTime, acId, alt\n",
    "  FROM pointtable\n",
    "  WHERE recTime>={} AND recTime<={}\n",
    "  \"\"\".format(t_start, t_end))\n",
    "\n",
    "spatialdf.createOrReplaceTempView(\"spatialdf\")\n",
    "spatialdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of points after the first query\n",
    "spatialdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Rectangular Query around KATL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katl = [33.6366996, -84.4278640, 10.06]  # https://www.airnav.com/airport/katl\n",
    "r = 0.2 # rectangular query range\n",
    "vs = 0.3 # vertical threshold unit: x100 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n",
    "range_query_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT DISTINCT acId\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]+vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of flight IDs after the second query\n",
    "range_query_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_query_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT *\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(katl[0]-r, katl[1]-r, katl[0]+r, katl[1]+r, katl[2]))\n",
    "\n",
    "# Count the number of points after the second query\n",
    "df_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymization\n",
    "\n",
    "Anonymization is a type of data sanitization technique to remove identifiable information from the data. In this work, we perform two operations to anonymize the data while retaining useful geo-spatial features.\n",
    "\n",
    "- Normalize the timestamp by the earliest time in the current dataframe.\n",
    "- Mask the real flight IDs into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create relevant timestamp column\n",
    "df_result.createOrReplaceTempView(\"spatialdf\")\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT acId, recTime-{} AS t, geom, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(t_start)\n",
    ")\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change acId Into integers\n",
    "# have to use pyspark ml features\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"acId\", outputCol=\"FacId\")\n",
    "df_new = indexer.fit(df).transform(df).drop('acId')\n",
    "df_new.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, CAST(FacId AS Integer), ST_X(geom) as lat, ST_Y(geom) as lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data after anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some small modifications to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 1: \n",
    "#### Resample the time series with an interval $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10\n",
    "t_start = 0\n",
    "t_end = 3600 * duration\n",
    "\n",
    "t_interval = list(range(t_start, t_end, dt))\n",
    "df = df[df.t.isin(t_interval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification 2:\n",
    "Change the origin of the coordinate system to the airport center\n",
    "- void in real case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"spatialdf\")\n",
    "\n",
    "df2 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, FacId, lat-{} AS Lat, lon-{} AS Lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(katl[0], katl[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data types are correct\n",
    "df.toPandas()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data with altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}_wAltitude.csv'.format(r, date, duration)\n",
    "df.toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')\n",
    "\n",
    "# save data without altitude dimension\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}.csv'.format(r, date, duration)\n",
    "df.drop('alt').toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "# df.coalesce(1).write.csv(csv_name, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
