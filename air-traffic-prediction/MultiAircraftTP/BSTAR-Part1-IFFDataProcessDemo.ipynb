{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spatio-Temporal Graph Transformer Network (B-STAR) for Multi-Aircraft Trajectory Prediction\n",
    "Author: Yutian Pang, Arizona State University\n",
    "\n",
    "\n",
    "Email: yutian.pang@asu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: IFF ASDE-X Flight Track Data Processing with PySpark and Hadoop\n",
    "This is a demonstration of using PySpark and Hadoop for large-scale processing of IFF ASDE-X data. In practice, this data processing would be performed on a server or high-performance cluster via ssh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Requirements\n",
    "\n",
    "This Jupyter notebook has been tested with:\n",
    "- Ubuntu 20.04 LTS (and 18.04 LTS)\n",
    "- Python 3.8.5 (and 3.8.10)\n",
    "- Spark 3.1.1 with Hadoop3.2 (and Spark 3.2.1 with Hadoop3.2)\n",
    "\n",
    "The software in parenthesis were tested together. Other combinations of Ubuntu, Python, and Spark should be verified for compatibility. See [this](https://stackoverflow.com/questions/58700384/how-to-fix-typeerror-an-integer-is-required-got-type-bytes-error-when-tryin) article for further guidance.\n",
    "\n",
    "### Instructions for Windows 10 Users\n",
    "\n",
    "1. **Install Ubuntu on Windows 10 with Windows Subsystem for LInux (WSL)**\n",
    "    - Windows 10 users with admin privileges can enable Windows Subsystem for Linux (WSL) following [these](https://docs.microsoft.com/en-us/windows/wsl/install-win10) directions.\n",
    "        \n",
    "        \n",
    "2. **Install Anaconda in the Ubuntu terminal**\n",
    "    - A user can then install Anaconda on their WSL Ubuntu distribution following [these](https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da) instructions. \n",
    "        \n",
    "        \n",
    "3. **Download and unzip Spark on WSL**\n",
    "    - Identify the distribution of Spark and Hadoop you require [here](https://spark.apache.org/downloads.html). \n",
    "    - In your Ubuntu terminal window execute the ```wget``` command followed by the download link in your chosen download directory (likely the ```HOME``` directory). \n",
    "    - Then, unzip the downloaded .tgz file with ```tar -xvzf [fname]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required Python packages\n",
    "The required Python packages for this module are:\n",
    "- **[```pyspark```]**(http://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "    - This is the Python API for Apache Spark. We will be using the distributed processing features and backend SQL queries for structured data.\n",
    "- **[```apache-sedona```]**(https://sedona.apache.org/)\n",
    "    - Formerly Geospark, Apache Sedona extends the Resilient Distributed Dataset (RDD), the core data structure in Apache Spark, to accommodate big geospatial data in a cluster environment.\n",
    "    \n",
    "In the Ubuntu or Anaconda terminal, execute ```pip install pyspark apache-sedona```. This will install both the ```pyspark``` and ```apache-sedona``` packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "The Spark codes (note: Improve this description) retrieve the ```SPARK_HOME```, ```PYTHONPATH```, ```PYSPARK_PYTHON```, and ```PYSPARK_DRIVER_PYTHON``` system variables. Either (Option 1) these are set in the shell environment in the ```.bash_profile``` script or (Option 2) in the Python script prior to calling the ```pyspark``` module.\n",
    "- ### Option 1: Add environment variables to the ```.bash_profile``` script\n",
    "\n",
    "    Open the ```.bash_profile``` script in your text editor. On Ubuntu systems, this script is usually found in your ```HOME``` directory ```~/```. If this file does not yet exist (or is empty) you can create one. Then add the following ```export``` statements for each variable you want to add and add them to the path. For example:\n",
    "\n",
    "    ```export SPARK_HOME=\"$HOME/spark-X.X.X-bin-hadoopX.X\"```\n",
    "\n",
    "    ```export PYTHONPATH=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_DRIVER_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PATH=\"$SPARK_HOME/bin:$PATH\"```\n",
    "\n",
    "- ### Option 2: Add the environment variables in the Python script using the ```os``` package\n",
    "\n",
    "    ```import os```\n",
    "       \n",
    "    ```os.environ[\"SPARK_HOME\"] = '~/spark-3.1.1-bin-hadoop3.2'```\n",
    "\n",
    "    ```os.environ[\"PYTHONPATH\"] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_PYTHON'] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_DRIVER_PYTHON'] = '~/anaconda3/bin/python3.8'```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 1: Loading IFF ASDE-X Data into the Python Environment\n",
    "### Step 1a: Use ```sedona``` to register ```SparkSession``` with geospatial packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"Sector_IFF_Parser\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "        config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.1-incubating,'\n",
    "           'org.datasyslab:geotools-wrapper:1.1.0-25.2'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Define SQL Schema for IFF ASDE-X Data with ```pyspark``` Structures\n",
    "- Retrieve requried SQL types from ```pyspark```.\n",
    "- Create ```load_schema``` function which returns variable ```myschema``` specifically for IFF recType=3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required SQL types from pyspark\n",
    "from pyspark.sql.types import (ShortType, StringType, StructType,StructField,LongType, IntegerType, DoubleType)\n",
    "\n",
    "#Create load_schema function that returns variable 'myschema' specifically for IFF recType=3.\n",
    "def load_schema():\n",
    "    myschema = StructType([\n",
    "        StructField(\"recType\", ShortType(), True),  # 1  //track point record type number\n",
    "        StructField(\"recTime\", StringType(), True),  # 2  //seconds since midnigght 1/1/70 UTC\n",
    "        StructField(\"fltKey\", LongType(), True),  # 3  //flight key\n",
    "        StructField(\"bcnCode\", IntegerType(), True),  # 4  //digit range from 0 to 7\n",
    "        StructField(\"cid\", IntegerType(), True),  # 5  //computer flight id\n",
    "        StructField(\"Source\", StringType(), True),  # 6  //source of the record\n",
    "        StructField(\"msgType\", StringType(), True),  # 7\n",
    "        StructField(\"acId\", StringType(), True),  # 8  //call sign\n",
    "        StructField(\"recTypeCat\", IntegerType(), True),  # 9\n",
    "        StructField(\"lat\", DoubleType(), True),  # 10\n",
    "        StructField(\"lon\", DoubleType(), True),  # 11\n",
    "        StructField(\"alt\", DoubleType(), True),  # 12  //in 100s of feet\n",
    "        StructField(\"significance\", ShortType(), True),  # 13 //digit range from 1 to 10\n",
    "        StructField(\"latAcc\", DoubleType(), True),  # 14\n",
    "        StructField(\"lonAcc\", DoubleType(), True),  # 15\n",
    "        StructField(\"altAcc\", DoubleType(), True),  # 16\n",
    "        StructField(\"groundSpeed\", IntegerType(), True),  # 17 //in knots\n",
    "        StructField(\"course\", DoubleType(), True),  # 18  //in degrees from true north\n",
    "        StructField(\"rateOfClimb\", DoubleType(), True),  # 19  //in feet per minute\n",
    "        StructField(\"altQualifier\", StringType(), True),  # 20  //Altitude qualifier (the “B4 character”)\n",
    "        StructField(\"altIndicator\", StringType(), True),  # 21  //Altitude indicator (the “C4 character”)\n",
    "        StructField(\"trackPtStatus\", StringType(), True),  # 22  //Track point status (e.g., ‘C’ for coast)\n",
    "        StructField(\"leaderDir\", IntegerType(), True),  # 23  //int 0-8 representing the direction of the leader line\n",
    "        StructField(\"scratchPad\", StringType(), True),  # 24\n",
    "        StructField(\"msawInhibitInd\", ShortType(), True),  # 25 // MSAW Inhibit Indicator (0=not inhibited, 1=inhibited)\n",
    "        StructField(\"assignedAltString\", StringType(), True),  # 26\n",
    "        StructField(\"controllingFac\", StringType(), True),  # 27\n",
    "        StructField(\"controllingSec\", StringType(), True),  # 28\n",
    "        StructField(\"receivingFac\", StringType(), True),  # 29\n",
    "        StructField(\"receivingSec\", StringType(), True),  # 30\n",
    "        StructField(\"activeContr\", IntegerType(), True),  # 31  // the active control number\n",
    "        StructField(\"primaryContr\", IntegerType(), True),\n",
    "        # 32  //The primary(previous, controlling, or possible next)controller number\n",
    "        StructField(\"kybrdSubset\", StringType(), True),  # 33  //identifies a subset of controller keyboards\n",
    "        StructField(\"kybrdSymbol\", StringType(), True),  # 34  //identifies a keyboard within the keyboard subsets\n",
    "        StructField(\"adsCode\", IntegerType(), True),  # 35  //arrival departure status code\n",
    "        StructField(\"opsType\", StringType(), True),  # 36  //Operations type (O/E/A/D/I/U)from ARTS and ARTS 3A data\n",
    "        StructField(\"airportCode\", StringType(), True),  # 37\n",
    "        StructField(\"trackNumber\", IntegerType(), True),  # 38\n",
    "        StructField(\"tptReturnType\", StringType(), True),  # 39\n",
    "        StructField(\"modeSCode\", StringType(), True)  # 40\n",
    "    ])\n",
    "    return myschema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1c: Load dataframe structured by ```iff_schema``` with ```spark```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../../miscellaneous/gnats-fpgen/IFF_SFO_ASDEX_ABC456.csv\"\n",
    "iff_schema = load_schema()\n",
    "df = spark.read.csv(fname, header=False, sep=\",\", schema=iff_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 2: Build geospatial dataframe with ```sedona``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Downselect features of ```recType```, ```recTime```, ```acId```, ```lat```,```lon```,```alt```\n",
    "In this work, we are simply interested in flight ID, timestamps, and coordinates (latitude, longitude, and altitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['recType', 'recTime', 'acId', 'lat', 'lon', 'alt']\n",
    "df = df.select(*cols).filter(df['recType']==3).withColumn(\"recTime\", df['recTime'].cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Register geospatial dataframe in SQL\n",
    "We need to register an SQL table that runs in the backend of the system for fast querying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"pointtable\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 3: Perform fast SQL queries to retrieve data subsets\n",
    "Now can use ```spark.sql``` commands to query from new registered table ```pointtable``` and create new data frames and register them as SQL tables. \n",
    "### Step 3a: Temporal queries of IFF ASDE-X data\n",
    "- Define desired time window from a starting timestamp (e.g. Monday, December 31, 2018 at 4:25pm PST)\n",
    "- Query returning all flight records within **1 hour** time window\n",
    "- Register query as SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define desired time window\n",
    "duration = 1 #hour\n",
    "t_start = 1546302340 #Monday, December 31, 2018 at 4:25pm in PST\n",
    "t_end = t_start + 3600*duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query returning all flight records within 1 hour time window\n",
    "temporal_df = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT ST_Point(CAST(lat AS Decimal(24, 20)), CAST(lon AS Decimal(24, 20))) AS geom, recTime, acId, alt\n",
    "  FROM pointtable\n",
    "  WHERE recTime>={} AND recTime<={}\n",
    "  \"\"\".format(t_start, t_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register query as SQL table\n",
    "temporal_df.createOrReplaceTempView(\"temporaldf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Spatial queries of IFF ASDE-X data\n",
    "- Define desired spatial rectangle around a central point (e.g. KSFO airport) in degrees and vertical feet\n",
    "- Query returning all **flight IDs** within registered ```temporaldf``` SQL table around KSFO airport using ```ST_PolygonFromEnvelope``` function from ```sedona```\n",
    "- Query returning all **flight records** again using ```ST_PolygonFromEnvelope``` function from ```sedona```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define desired spatial rectangle around a central point (e.g. KSFO airport) \n",
    "apt_coords = [37.6188056,-122.3754167, 0]  # from https://www.airnav.com/airport/ksfo\n",
    "r = 0.2 # rectangular query range unit: degrees\n",
    "vs = 0.3 # vertical threshold unit: x100 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query returning all flight IDs within temporal_df around KSFO\n",
    "range_query_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT DISTINCT acId\n",
    "    FROM temporaldf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(apt_coords[0]-r, apt_coords[1]-r, apt_coords[0]+r, apt_coords[1]+r, apt_coords[2]+vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query returning all flight records within temporal_df around KSFO\n",
    "df_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT *\n",
    "    FROM spatialdf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(apt_coords[0]-r, apt_coords[1]-r, apt_coords[0]+r, apt_coords[1]+r, apt[2]+vs))\n",
    "\n",
    "# Count the number of points after the second query\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 4: Data Anonymization and Sanitation\n",
    "\n",
    "Anonymization is a type of data sanitization technique to remove identifiable information from sensitive data. Here, we perform two operations to anonymize the data while retaining useful geo-spatial features.\n",
    "\n",
    "- Normalize the timestamp by the earliest time in the current dataframe.\n",
    "- Mask the real flight IDs into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the timestamp by the earlierst time in the current dataframe\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT acId, recTime-{} AS t, geom, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(t_start)\n",
    ")\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mask the real flight IDs into integers\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"acId\", outputCol=\"FacId\")\n",
    "df_new = indexer.fit(df).transform(df).drop('acId')\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Manipulations of the Data for BSTAR Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 1. Resample the time series with an interval ```dt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10\n",
    "t_start = 0\n",
    "t_end = 3600 * duration\n",
    "\n",
    "t_interval = list(range(t_start, t_end, dt))\n",
    "df = df[df.t.isin(t_interval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 2. Change the origin of the coordinate system to the airport center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, FacId, lat-{} AS Lat, lon-{} AS Lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(apt_coords[0], apt_coords[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 3: Save dataframe into csv\n",
    "- Convert dataframe to ```pandas```\n",
    "- Save data using the ```to_csv``` function in ```pandas```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data types are correct\n",
    "df.toPandas()\n",
    "df.dtypes #Make sure the data types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data with altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}_wAltitude.csv'.format(r, t_start, duration)\n",
    "df.toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "\n",
    "# Save data without altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}.csv'.format(r, t_start, duration)\n",
    "df.drop('alt').toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
