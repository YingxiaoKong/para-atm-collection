{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spatio-Temporal Graph Transformer Network (B-STAR) for Multi-Aircraft Trajectory Prediction\n",
    "Author: Yutian Pang, Arizona State University\n",
    "\n",
    "\n",
    "Email: yutian.pang@asu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: IFF ASDE-X Flight Track Data Processing with PySpark and Hadoop\n",
    "This is a demonstration of using PySpark and Hadoop for large-scale processing of IFF ASDE-X data. In practice, this data processing would be performed on a server or high-performance cluster via ssh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Requirements\n",
    "\n",
    "This Jupyter notebook has been tested with:\n",
    "- Ubuntu 20.04 LTS (and 18.04 LTS)\n",
    "- Python 3.8.5 (and 3.8.10)\n",
    "- Spark 3.1.1 with Hadoop3.2 (and Spark 3.2.1 with Hadoop3.2)\n",
    "\n",
    "The software in parenthesis were tested together. Other combinations of Ubuntu, Python, and Spark should be verified for compatibility. See [this](https://stackoverflow.com/questions/58700384/how-to-fix-typeerror-an-integer-is-required-got-type-bytes-error-when-tryin) article for further guidance.\n",
    "\n",
    "### Instructions for Windows 10 Users\n",
    "\n",
    "1. **Install Ubuntu on Windows 10 with Windows Subsystem for LInux (WSL)**\n",
    "    - Windows 10 users with admin privileges can enable Windows Subsystem for Linux (WSL) following [these](https://docs.microsoft.com/en-us/windows/wsl/install-win10) directions.\n",
    "        \n",
    "        \n",
    "2. **Install Anaconda in the Ubuntu terminal**\n",
    "    - A user can then install Anaconda on their WSL Ubuntu distribution following [these](https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da) instructions. \n",
    "        \n",
    "        \n",
    "3. **Download and unzip Spark on WSL**\n",
    "    - Identify the distribution of Spark and Hadoop you require [here](https://spark.apache.org/downloads.html). \n",
    "    - In your Ubuntu terminal window execute the ```wget``` command followed by the download link in your chosen download directory (likely the ```HOME``` directory). \n",
    "    - Then, unzip the downloaded .tgz file with ```tar -xvzf [fname]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required Python packages\n",
    "The required Python packages for this module are:\n",
    "- **[```pyspark```]**(http://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "    - This is the Python API for Apache Spark. We will be using the distributed processing features and backend SQL queries for structured data.\n",
    "- **[```apache-sedona```]**(https://sedona.apache.org/)\n",
    "    - Formerly Geospark, Apache Sedona extends the Resilient Distributed Dataset (RDD), the core data structure in Apache Spark, to accommodate big geospatial data in a cluster environment.\n",
    "    \n",
    "In the Ubuntu or Anaconda terminal, execute ```pip install pyspark apache-sedona```. This will install both the ```pyspark``` and ```apache-sedona``` packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "The Spark codes (note: Improve this description) retrieve the ```SPARK_HOME```, ```PYTHONPATH```, ```PYSPARK_PYTHON```, and ```PYSPARK_DRIVER_PYTHON``` system variables. Either (Option 1) these are set in the shell environment in the ```.bash_profile``` script or (Option 2) in the Python script prior to calling the ```pyspark``` module.\n",
    "- ### Option 1: Add environment variables to the ```.bash_profile``` script\n",
    "\n",
    "    Open the ```.bash_profile``` script in your text editor. On Ubuntu systems, this script is usually found in your ```HOME``` directory ```~/```. If this file does not yet exist (or is empty) you can create one. Then add the following ```export``` statements for each variable you want to add and add them to the path. For example:\n",
    "\n",
    "    ```export SPARK_HOME=\"$HOME/spark-X.X.X-bin-hadoopX.X\"```\n",
    "\n",
    "    ```export PYTHONPATH=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PYSPARK_DRIVER_PYTHON=\"$HOME/anacond3/bin/python3.8\"```\n",
    "\n",
    "    ```export PATH=\"$SPARK_HOME/bin:$PATH\"```\n",
    "\n",
    "- ### Option 2: Add the environment variables in the Python script using the ```os``` package\n",
    "\n",
    "    ```import os```\n",
    "       \n",
    "    ```os.environ[\"SPARK_HOME\"] = '~/spark-3.1.1-bin-hadoop3.2'```\n",
    "\n",
    "    ```os.environ[\"PYTHONPATH\"] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_PYTHON'] = '~/anaconda3/bin/python3.8'```\n",
    "\n",
    "    ```os.environ['PYSPARK_DRIVER_PYTHON'] = '~/anaconda3/bin/python3.8'```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 1: Loading IFF ASDE-X Data into the Python Environment\n",
    "### Step 1a: Use ```sedona``` to register ```SparkSession``` with geospatial packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/02 11:22:29 WARN Utils: Your hostname, EDECARLO-LT-1 resolves to a loopback address: 127.0.1.1; using 172.30.15.228 instead (on interface eth0)\n",
      "22/02/02 11:22:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/edecarlo/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/edecarlo/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/edecarlo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/edecarlo/.ivy2/jars\n",
      "org.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0050eea1-420d-4628-952a-b187f1c4e12f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.1-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.apache.sedona#sedona-core-3.0_2.12;1.1.1-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.1.1-incubating in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      ":: resolution report :: resolve 677ms :: artifacts dl 56ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-core-3.0_2.12;1.1.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.1.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-3.0_2.12;1.1.1-incubating from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0050eea1-420d-4628-952a-b187f1c4e12f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/22ms)\n",
      "22/02/02 11:22:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"Sector_IFF_Parser\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "        config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.1-incubating,'\n",
    "           'org.datasyslab:geotools-wrapper:1.1.0-25.2'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Define SQL Schema for IFF ASDE-X Data with ```pyspark``` Structures\n",
    "- Retrieve requried SQL types from ```pyspark```.\n",
    "- Create ```load_schema``` function which returns variable ```myschema``` specifically for IFF recType=3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required SQL types from pyspark\n",
    "from pyspark.sql.types import (ShortType, StringType, StructType,StructField,LongType, IntegerType, DoubleType)\n",
    "\n",
    "#Create load_schema function that returns variable 'myschema' specifically for IFF recType=3.\n",
    "def load_schema():\n",
    "    myschema = StructType([\n",
    "        StructField(\"recType\", ShortType(), True),  # 1  //track point record type number\n",
    "        StructField(\"recTime\", StringType(), True),  # 2  //seconds since midnigght 1/1/70 UTC\n",
    "        StructField(\"fltKey\", LongType(), True),  # 3  //flight key\n",
    "        StructField(\"bcnCode\", IntegerType(), True),  # 4  //digit range from 0 to 7\n",
    "        StructField(\"cid\", IntegerType(), True),  # 5  //computer flight id\n",
    "        StructField(\"Source\", StringType(), True),  # 6  //source of the record\n",
    "        StructField(\"msgType\", StringType(), True),  # 7\n",
    "        StructField(\"acId\", StringType(), True),  # 8  //call sign\n",
    "        StructField(\"recTypeCat\", IntegerType(), True),  # 9\n",
    "        StructField(\"lat\", DoubleType(), True),  # 10\n",
    "        StructField(\"lon\", DoubleType(), True),  # 11\n",
    "        StructField(\"alt\", DoubleType(), True),  # 12  //in 100s of feet\n",
    "        StructField(\"significance\", ShortType(), True),  # 13 //digit range from 1 to 10\n",
    "        StructField(\"latAcc\", DoubleType(), True),  # 14\n",
    "        StructField(\"lonAcc\", DoubleType(), True),  # 15\n",
    "        StructField(\"altAcc\", DoubleType(), True),  # 16\n",
    "        StructField(\"groundSpeed\", IntegerType(), True),  # 17 //in knots\n",
    "        StructField(\"course\", DoubleType(), True),  # 18  //in degrees from true north\n",
    "        StructField(\"rateOfClimb\", DoubleType(), True),  # 19  //in feet per minute\n",
    "        StructField(\"altQualifier\", StringType(), True),  # 20  //Altitude qualifier (the “B4 character”)\n",
    "        StructField(\"altIndicator\", StringType(), True),  # 21  //Altitude indicator (the “C4 character”)\n",
    "        StructField(\"trackPtStatus\", StringType(), True),  # 22  //Track point status (e.g., ‘C’ for coast)\n",
    "        StructField(\"leaderDir\", IntegerType(), True),  # 23  //int 0-8 representing the direction of the leader line\n",
    "        StructField(\"scratchPad\", StringType(), True),  # 24\n",
    "        StructField(\"msawInhibitInd\", ShortType(), True),  # 25 // MSAW Inhibit Indicator (0=not inhibited, 1=inhibited)\n",
    "        StructField(\"assignedAltString\", StringType(), True),  # 26\n",
    "        StructField(\"controllingFac\", StringType(), True),  # 27\n",
    "        StructField(\"controllingSec\", StringType(), True),  # 28\n",
    "        StructField(\"receivingFac\", StringType(), True),  # 29\n",
    "        StructField(\"receivingSec\", StringType(), True),  # 30\n",
    "        StructField(\"activeContr\", IntegerType(), True),  # 31  // the active control number\n",
    "        StructField(\"primaryContr\", IntegerType(), True),\n",
    "        # 32  //The primary(previous, controlling, or possible next)controller number\n",
    "        StructField(\"kybrdSubset\", StringType(), True),  # 33  //identifies a subset of controller keyboards\n",
    "        StructField(\"kybrdSymbol\", StringType(), True),  # 34  //identifies a keyboard within the keyboard subsets\n",
    "        StructField(\"adsCode\", IntegerType(), True),  # 35  //arrival departure status code\n",
    "        StructField(\"opsType\", StringType(), True),  # 36  //Operations type (O/E/A/D/I/U)from ARTS and ARTS 3A data\n",
    "        StructField(\"airportCode\", StringType(), True),  # 37\n",
    "        StructField(\"trackNumber\", IntegerType(), True),  # 38\n",
    "        StructField(\"tptReturnType\", StringType(), True),  # 39\n",
    "        StructField(\"modeSCode\", StringType(), True)  # 40\n",
    "    ])\n",
    "    return myschema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1c: Load dataframe structured by ```iff_schema``` with ```spark```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../../miscellaneous/gnats-fpgen/IFF_SFO_ASDEX_ABC456.csv\"\n",
    "iff_schema = load_schema()\n",
    "df = spark.read.csv(fname, header=False, sep=\",\", schema=iff_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 2: Build geospatial dataframe with ```sedona``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Downselect features of ```recType```, ```recTime```, ```acId```, ```lat```,```lon```,```alt```\n",
    "In this work, we are simply interested in flight ID, timestamps, and coordinates (latitude, longitude, and altitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+----------+----+\n",
      "|recType|   recTime|  acId|     lat|       lon| alt|\n",
      "+-------+----------+------+--------+----------+----+\n",
      "|      3|1546302315|ABC123|37.61867|-122.38173|0.06|\n",
      "|      3|1546302316|ABC123| 37.6187|-122.38171|0.06|\n",
      "|      3|1546302318|ABC123|37.61874|-122.38169|0.06|\n",
      "|      3|1546302319|ABC123|37.61876|-122.38172|0.06|\n",
      "|      3|1546302320|ABC123|37.61878|-122.38173|0.06|\n",
      "+-------+----------+------+--------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['recType', 'recTime', 'acId', 'lat', 'lon', 'alt']\n",
    "df = df.select(*cols).filter(df['recType']==3).withColumn(\"recTime\", df['recTime'].cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Register geospatial dataframe in SQL\n",
    "We need to register an SQL table that runs in the backend of the system for fast querying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/02 11:22:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+----------+----+\n",
      "|recType|recTime   |acId  |lat     |lon       |alt |\n",
      "+-------+----------+------+--------+----------+----+\n",
      "|3      |1546302315|ABC123|37.61867|-122.38173|0.06|\n",
      "|3      |1546302316|ABC123|37.6187 |-122.38171|0.06|\n",
      "|3      |1546302318|ABC123|37.61874|-122.38169|0.06|\n",
      "|3      |1546302319|ABC123|37.61876|-122.38172|0.06|\n",
      "|3      |1546302320|ABC123|37.61878|-122.38173|0.06|\n",
      "+-------+----------+------+--------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"pointtable\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 3: Perform fast SQL queries to retrieve data subsets\n",
    "Now can use ```spark.sql``` commands to query from new registered table ```pointtable``` and create new data frames and register them as SQL tables. \n",
    "### Step 3a: Temporal queries of IFF ASDE-X data\n",
    "- Define desired time window from a starting timestamp (e.g. Monday, December 31, 2018 at 4:25pm PST)\n",
    "- Query returning all flight records within **1 hour** time window\n",
    "- Register query as SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define desired time window\n",
    "duration = 1 #hour\n",
    "t_start = 1546302340 #Monday, December 31, 2018 at 4:25pm in PST\n",
    "t_end = t_start + 3600*duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query returning all flight records within 1 hour time window\n",
    "temporal_df = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT ST_Point(CAST(lat AS Decimal(24, 20)), CAST(lon AS Decimal(24, 20))) AS geom, recTime, acId, alt\n",
    "  FROM pointtable\n",
    "  WHERE recTime>={} AND recTime<={}\n",
    "  \"\"\".format(t_start, t_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register query as SQL table\n",
    "temporal_df.createOrReplaceTempView(\"temporaldf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Spatial queries of IFF ASDE-X data\n",
    "- Define desired spatial rectangle around a central point (e.g. KSFO airport) in degrees and vertical feet\n",
    "- Query returning all **flight IDs** within registered ```temporaldf``` SQL table around KSFO airport using ```ST_PolygonFromEnvelope``` function from ```sedona```\n",
    "- Query returning all **flight records** again using ```ST_PolygonFromEnvelope``` function from ```sedona```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define desired spatial rectangle around a central point (e.g. KSFO airport) \n",
    "apt_coords = [37.6188056,-122.3754167, 0]  # from https://www.airnav.com/airport/ksfo\n",
    "r = 0.2 # rectangular query range unit: degrees\n",
    "vs = 0.3 # vertical threshold unit: x100 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query returning all flight IDs within temporal_df around KSFO\n",
    "range_query_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT DISTINCT acId\n",
    "    FROM temporaldf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(apt_coords[0]-r, apt_coords[1]-r, apt_coords[0]+r, apt_coords[1]+r, apt_coords[2]+vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  acId|\n",
      "+------+\n",
      "|ABC123|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "range_query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+----+\n",
      "|                geom|   recTime|  acId| alt|\n",
      "+--------------------+----------+------+----+\n",
      "|POINT (37.62887 -...|1546303252|ABC123|1.13|\n",
      "|POINT (37.62963 -...|1546303253|ABC123|1.44|\n",
      "|POINT (37.63041 -...|1546303254|ABC123|1.63|\n",
      "|POINT (37.63116 -...|1546303255|ABC123|2.19|\n",
      "|POINT (37.63192 -...|1546303256|ABC123|2.69|\n",
      "+--------------------+----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Query returning all flight records within temporal_df around KSFO\n",
    "df_result = spark.sql(\n",
    "  \"\"\"\n",
    "    SELECT *\n",
    "    FROM temporaldf\n",
    "    WHERE ST_Contains(ST_PolygonFromEnvelope({}, {}, {}, {}), geom) AND alt>{}\n",
    "  \"\"\".format(apt_coords[0]-r, apt_coords[1]-r, apt_coords[0]+r, apt_coords[1]+r, apt_coords[2]+vs))\n",
    "\n",
    "# Count the number of points after the second query\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 4: Data Anonymization and Sanitation\n",
    "\n",
    "Anonymization is a type of data sanitization technique to remove identifiable information from sensitive data. Here, we perform two operations to anonymize the data while retaining useful geo-spatial features.\n",
    "\n",
    "- Normalize the timestamp by the earliest time in the current dataframe.\n",
    "- Mask the real flight IDs into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the timestamp by the earlierst time in the current dataframe\n",
    "df = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT acId, recTime-{} AS t, geom, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(t_start)\n",
    ")\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----------+----+-----+\n",
      "|recType|   recTime|     lat|       lon| alt|FacId|\n",
      "+-------+----------+--------+----------+----+-----+\n",
      "|      3|1546302315|37.61867|-122.38173|0.06|  1.0|\n",
      "|      3|1546302316| 37.6187|-122.38171|0.06|  1.0|\n",
      "|      3|1546302318|37.61874|-122.38169|0.06|  1.0|\n",
      "|      3|1546302319|37.61876|-122.38172|0.06|  1.0|\n",
      "|      3|1546302320|37.61878|-122.38173|0.06|  1.0|\n",
      "+-------+----------+--------+----------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Mask the real flight IDs into integers\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"acId\", outputCol=\"FacId\")\n",
    "df_new = indexer.fit(df).transform(df).drop('acId')\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Manipulations of the Data for BSTAR Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 1. Resample the time series with an interval ```dt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m t_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m*\u001b[39m duration\n\u001b[1;32m      5\u001b[0m t_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(t_start, t_end, dt))\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241m.\u001b[39misin(t_interval)]\n",
      "File \u001b[0;32m~/anaconda3/envs/paraatm/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1400\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \n\u001b[1;32m   1396\u001b[0m \u001b[38;5;124;03m>>> df.select(df.age).collect()\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1402\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 't'"
     ]
    }
   ],
   "source": [
    "dt = 10\n",
    "t_start = 0\n",
    "t_end = 3600 * duration\n",
    "\n",
    "t_interval = list(range(t_start, t_end, dt))\n",
    "df = df[df.t.isin(t_interval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+----------+----+\n",
      "|recType|   recTime|  acId|     lat|       lon| alt|\n",
      "+-------+----------+------+--------+----------+----+\n",
      "|      3|1546302315|ABC123|37.61867|-122.38173|0.06|\n",
      "|      3|1546302316|ABC123| 37.6187|-122.38171|0.06|\n",
      "|      3|1546302318|ABC123|37.61874|-122.38169|0.06|\n",
      "|      3|1546302319|ABC123|37.61876|-122.38172|0.06|\n",
      "|      3|1546302320|ABC123|37.61878|-122.38173|0.06|\n",
      "+-------+----------+------+--------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 2. Change the origin of the coordinate system to the airport center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT t, FacId, lat-{} AS Lat, lon-{} AS Lon, alt\n",
    "    FROM spatialdf\n",
    "\"\"\".format(apt_coords[0], apt_coords[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation 3: Save dataframe into csv\n",
    "- Convert dataframe to ```pandas```\n",
    "- Save data using the ```to_csv``` function in ```pandas```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data types are correct\n",
    "df.toPandas()\n",
    "df.dtypes #Make sure the data types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data with altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}_wAltitude.csv'.format(r, t_start, duration)\n",
    "df.toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)\n",
    "\n",
    "# Save data without altitude\n",
    "csv_name = 'KATL_r_{}_date_{}_range_{}.csv'.format(r, t_start, duration)\n",
    "df.drop('alt').toPandas().T.to_csv(csv_name, sep=',', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
