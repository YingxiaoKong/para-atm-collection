{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggressive-victorian",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network for touchdown vertical speed prediciton\n",
    "Author: Yingxiao Kong, Vanderbilt University\n",
    "\n",
    "Email: kongyingxiao@gmail.com\n",
    "\n",
    "This module demonstrates how to build a Bayesian Neural Network to predict touchdown vertical speed. \n",
    "\n",
    "More information about Bayesian Neural Network can be found [here](https://proceedings.neurips.cc/paper/2016/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-durham",
   "metadata": {},
   "source": [
    "## Code Requirements\n",
    "\n",
    "The requirments for this module are **DASHlink** data whihc can be found [here](https://c3.nasa.gov/dashlink/resources/).\n",
    "\n",
    "**Keras** is required to be installed in advance to run this code.  \n",
    "\n",
    "This code has been tested under python *3.6* and keras *2.3.1*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "honest-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout,LSTM,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "stopped-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork():\n",
    "    '''\n",
    "    BayesianNeuralNetwork class creates a Bayesian Neural Network to predict touchdown vertical speed \n",
    "    with model uncertainty quantified.\n",
    "      \n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    get_rnn_model\n",
    "    get_dnn_model\n",
    "    custom_model\n",
    "    bnn_pred\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "        \n",
    "    def get_rnn_model(self,x_train,y_train, idrop=0.,\n",
    "                 odrop=0.25,rdrop=0.25,\n",
    "                 weight_decay=1e-4,lr=1e-3,num_unit=100):\n",
    "        '''\n",
    "        Initialize the RNN model.\n",
    "        \n",
    "        \n",
    "        Attributes\n",
    "        -----------        \n",
    "        x_train: training data\n",
    "        y_train: training target  \n",
    "        idrop: dropout rate for input layer\n",
    "        odrop: dropout rate for output layer\n",
    "        rdrop:dropout rate for the recurrent layer(DNN model doesn't need rdrop\n",
    "              but is provided by default)\n",
    "        weight_decay: regularization factor\n",
    "        lr: learning rate\n",
    "        num_unit: number of unit at each layer\n",
    "        '''\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.idrop = idrop\n",
    "        self.odrop = odrop\n",
    "        self.rdrop = rdrop\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "        self.num_unit=num_unit \n",
    "        \n",
    "        \n",
    "        if len(self.x_train.shape)!=3:\n",
    "            raise Exception('Expect a 3 dimensional array!')\n",
    "        in_shape = self.x_train.shape[-1]\n",
    "        model=Sequential()\n",
    "        model.add(LSTM(self.num_unit,kernel_regularizer=l2(self.weight_decay),\n",
    "                       recurrent_regularizer=l2(self.weight_decay),\n",
    "                       bias_regularizer=l2(self.weight_decay),dropout=self.idrop,\n",
    "                       recurrent_dropout=self.rdrop,input_shape=(None, in_shape),\n",
    "                      kernel_initializer='random_uniform',return_sequences=True))\n",
    "\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(LSTM(self.num_unit,dropout=self.idrop,\n",
    "                       recurrent_dropout=self.rdrop,return_sequences=False,\n",
    "                       kernel_regularizer=l2(self.weight_decay),\n",
    "                       recurrent_regularizer=l2(self.weight_decay),\n",
    "                       bias_regularizer=l2(self.weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        if self.odrop:\n",
    "            model.add(Dropout(self.odrop))\n",
    "        model.add(Dense(1,activation='linear',\n",
    "                        kernel_regularizer=l2(self.weight_decay),\n",
    "                        bias_regularizer=l2(self.weight_decay)))\n",
    "        optimizer_=Adam(self.lr)\n",
    "        model.compile(loss='mse',metrics=['mse'],optimizer=optimizer_)\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def get_dnn_model(self,x_train,y_train, idrop=0.,\n",
    "                 odrop=0.25,rdrop=0.25,\n",
    "                 weight_decay=1e-4,lr=1e-3,num_unit=100):\n",
    "        '''\n",
    "        Initialize the DNN model.\n",
    "        \n",
    "        Attributes\n",
    "        -----------        \n",
    "        x_train: training data\n",
    "        y_train: training target \n",
    "        idrop: dropout rate for input layer\n",
    "        odrop: dropout rate for output layer\n",
    "        rdrop:dropout rate for the recurrent layer(DNN model doesn't need rdrop\n",
    "        but is provided by default)\n",
    "        weight_decay: regularization factor\n",
    "        lr: learning rate\n",
    "        num_unit: number of unit at each layer\n",
    "        '''        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.idrop = idrop\n",
    "        self.odrop = odrop\n",
    "        self.rdrop = rdrop\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "        self.num_unit=num_unit  \n",
    "        self.model_type = 'DNN'\n",
    "        \n",
    "\n",
    "        if len(self.x_train.shape)!=2:\n",
    "            raise Exception('Expect a 2 dimensional array!')\n",
    "        in_shape = self.x_train.shape[-1]\n",
    "        model=Sequential()\n",
    "        model.add(Dense(self.num_unit,kernel_regularizer=l2(self.weight_decay),\n",
    "                       bias_regularizer=l2(self.weight_decay),\n",
    "                        kernel_initializer='random_uniform',\n",
    "                        input_dim=in_shape))\n",
    "        \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(self.idrop))\n",
    "        model.add(Dense(self.num_unit,kernel_regularizer=l2(self.weight_decay),\n",
    "                       bias_regularizer=l2(self.weight_decay),\n",
    "                        kernel_initializer='random_uniform',\n",
    "                       ))\n",
    "        model.add(Dropout(self.idrop))\n",
    "        model.add(Activation('relu'))  \n",
    "        if self.odrop:\n",
    "            model.add(Dropout(self.odrop))\n",
    "        model.add(Dense(1,activation='linear',\n",
    "                        kernel_regularizer=l2(self.weight_decay),\n",
    "                        bias_regularizer=l2(self.weight_decay)))\n",
    "        optimizer_=Adam(self.lr)\n",
    "        model.compile(loss='mse',metrics=['mse'],optimizer=optimizer_)  \n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def custom_model(self, model):\n",
    "        '''\n",
    "        This method allows you to use your own model, make sure dropout is included in your model, otherwise \n",
    "        prediction will be deterministic\n",
    "        \n",
    "        '''\n",
    "        self.model = model\n",
    "    \n",
    "    def bnn_pred(self,x_test,uncertainty = 'Y',batch_size=30,epochs=200,iter_=10):\n",
    "        \"\"\"\n",
    "        BDNpred is used to generate results for the deep learning model\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        x_test: test data\n",
    "        batch_size: mini batch size\n",
    "        epochs: number of epochs\n",
    "        iter_: number of predictions for each sample        \n",
    "        uncertainty: whether to quantity the uncertainty in the model or not;\n",
    "                    'Y': include uncertainty\n",
    "                    'N': not include unceratinty\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x_test = x_test\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.iter_ = iter_\n",
    "        self.uncertainty = uncertainty\n",
    "\n",
    "        if self.model==None:\n",
    "            raise Exception('A model is needed before calling bnn_pred!if you want to build a RNN model, you should first call get_rnn_model;if you want to build a DNN model, you should first call get_dnn_model;if you want to use your own model, you should first call custom_model.')\n",
    "        \n",
    "        #### fit the model        \n",
    "        self.model.fit(self.x_train,self.y_train,self.batch_size,\n",
    "                      self.epochs,verbose = False)\n",
    "        if self.uncertainty =='Y':\n",
    "            f = K.function([self.model.layers[0].input,\n",
    "                                K.learning_phase()],[self.model.layers[-1].output])\n",
    "            results = []\n",
    "            for i in range(self.iter_):\n",
    "                results.append(np.squeeze(f([self.x_test,1])))\n",
    "            results = np.array(results)\n",
    "        if self.uncertainty == 'N':\n",
    "            results = self.model.predict(self.x_test)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-closer",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "appreciated-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "### randomly generate some data for training \n",
    "\n",
    "RNN_x_train = RNN_x_test = np.random.rand(10,2,2)\n",
    "RNN_y_train = RNN_y_test = np.random.rand(10,)\n",
    "\n",
    "DNN_x_train = DNN_x_test = np.random.rand(10,2)\n",
    "DNN_y_train = DNN_y_test = np.random.rand(10,)\n",
    "\n",
    "### Initialize the class\n",
    "\n",
    "test = BayesianNeuralNetwork()\n",
    "\n",
    "### Initialize RNN model\n",
    "test.get_dnn_model(DNN_x_train,DNN_y_train)\n",
    "\n",
    "### make prediction for test data\n",
    "predictions = test.bnn_pred(DNN_x_test,uncertainty = 'Y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landing",
   "language": "python",
   "name": "landing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
